<!DOCTYPE html>
<html lang="en">
  <head>
    <title>HumanEval-V</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks">
    <meta name="keywords" content="Large Multimodal Model, Code Generation, Vision Language Model, Large Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks</title>

    <link rel="icon" href="static/images/square_icon.png">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    
    <script src="https://kit.fontawesome.com/eaf1856e6f.js" crossorigin="anonymous"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  </head>
  <body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title is-bold">
                <img src="static/images/icon.png" style="width:1em;vertical-align: middle" alt="Logo"/>
                <span style="vertical-align: middle">HumanEval-V</span>
              </h1>
              <h2 class="subtitle is-3 publication-subtitle">
                Benchmarking High-Level Visual Reasoning with <br> Complex Diagrams in Coding Tasks
              </h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">Fengji Zhang*<sup style="color:#6fbf73;">‚Ä†,1</sup></a>,</span>
                <span class="author-block">
                  Linquan Wu*<sup style="color:#6fbf73;">1</sup></a>,
                </span>
                <span class="author-block">
                  Bai Huiyu*<sup style="color:#6fbf73;">1</sup></a>,
                </span>
                <span class="author-block">Guancheng Lin*<sup style="color:#007bff;">2</sup>,</span><br>
                <span class="author-block">Xiao Li<sup style="color:#ffac33;">3</sup>,</span>
                <span class="author-block">Xiao Yu<sup style="color:#ed4b82;">4</sup>,</span>
                <span class="author-block">Yue Wang<sup style="color:#9b51e0;">5</sup>,</span>
                <span class="author-block">Bei Chen<sup style="color:#9b51e0;">5</sup>,</span>
                <span class="author-block">Jacky Keung<sup style="color:#6fbf73;">1</sup></span>
              </div>
              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#6fbf73;">1</sup>CityU Hong Kong,</span>
                <span class="author-block"><sup style="color:#007bff;">2</sup>Wuhan University,</span>
                <span class="author-block"><sup style="color:#ffac33;">3</sup>Tsinghua University,</span>
                <span class="author-block"><sup style="color:#ed4b82;">4</sup>Zhejiang University,</span>
                <span class="author-block"><sup style="color:#9b51e0;">5</sup>Rhymes AI</span>
              </div>

              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">‚Ä†Corresponding to:</span>
                <span class="author-block"><a href="mailto:notfinished@yet.com">fengji.zhang@my.cityu.edu.hk</a></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.12381" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>&nbsp
                  <span class="link-block">
                    <a href="https://github.com/HumanEval-V/HumanEval-V-Benchmark" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>&nbsp
                  <span class="link-block">
                    <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">üèÜ</span>
                      <span>Leaderboard</span>
                    </a>
                  </span>&nbsp
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>Dataset</span>
                    </a>
                  </span>&nbsp
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/HumanEval-V/HumanEval-V-Benchmark-Viewer" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>Dataset Viewer</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop has-text-centered">
        <img src="static/images/task_example.png" alt="HumanEval-V Coding Task"  width="65%"/>
        <p>An example task in HumanEval-V. Each task involves completing a <b>Python function</b> based on<br> <b>a single diagram, the function signature, and simple instructions</b> provided in the comment block.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">üëÄIntroduction</h2>
            <div class="content has-text-justified">
              <p><strong>HumanEval-V</strong> is a novel benchmark designed to evaluate the ability of <strong>Large Multimodal Models (LMMs)</strong> to understand and reason over <strong>complex diagrams</strong> in programming contexts. Unlike traditional multimodal or coding benchmarks, <strong>HumanEval-V</strong> challenges models to generate <strong>Python code</strong> based on <strong>visual inputs</strong> that are <strong>indispensable</strong> for solving the task. Our dataset consists of <strong>253 human-annotated coding tasks</strong>, each requiring LMMs to <strong>perceive, interpret, and reason</strong> over diagrams to produce functionally correct code solutions.</p>

              <h5><strong>Why HumanEval-V?</strong></h5>

              <p>Despite recent advancements in <strong>multimodal reasoning</strong>, existing benchmarks focus primarily on <strong>scientific</strong>, <strong>mathematical</strong>, <strong>chart-based analysis</strong>, or <strong>abstract visual reasoning</strong> (in IQ tests), assessing models' domain knowledge or deduction abilities. These benchmarks don't fully challenge models to understand <strong>complex diagrams</strong> in the way that humans do.</p>

              <p><strong>HumanEval-V</strong> addresses this gap by introducing coding tasks where the <strong>diagram alone</strong> encodes most of the problem context. Models must perform advanced <strong>visual reasoning</strong> without relying on lengthy textual descriptions, pushing the boundaries of <strong>vision reasoning</strong> capabilities.</p>

              <h5><strong>Key Features</strong></h5>
              <div class="container is-max-desktop has-text-centered">
                <img src="static/images/task_type_and_capability_aspects.png" alt="Task types in HumanEval-V, and the capability aspects required for understanding diagrams in HumanEval-V"  width="100%"/>
              </div>

              <ul>
                <li><strong>Indispensable visual context</strong>: Each task includes a self-contained diagram, eliminating reliance on detailed textual descriptions.</li>
                <li><strong>Diverse and realistic problem types</strong>: The dataset spans <strong>six distinct categories</strong>, covering a wide range of visual reasoning abilities.</li>
                <li><strong>Code generation task</strong>: Unlike many multimodal benchmarks, which rely on <strong>MCQ or short-answer tasks</strong>, HumanEval-V requires models to <strong>generate executable code</strong>, ensuring a more rigorous evaluation of diagram comprehension.</li>
                <li><strong>Structured evaluation pipeline</strong>: We introduce a <strong>two-stage evaluation</strong> approach where LMMs only need to generate a structured <strong>diagram description</strong>, which will be translated into code by a separate strong coder model. This ensures that visual understanding is explicitly assessed rather than conflated with coding proficiency.</li>
                <li><strong>Execution-based evaluation</strong>: Solutions are tested using <strong>handcrafted test cases</strong> and scored with the <strong>pass@k</strong> metric, providing an objective measure of correctness.</li>
              </ul>

              <h5><strong>Challenges for LMMs</strong></h5>
              <ul>
                <li><strong>Top-performing models</strong> struggle, with <strong>Claude 3.5 Sonnet</strong> achieving only <strong>36.8% pass@1</strong>, while <strong>Pixtral 124B</strong> reaches <strong>21.3% pass@1</strong>.</li>
                <li>LMMs perform better at <strong>diagram description</strong> than direct <strong>code generation</strong>, revealing a gap in their <strong>vision-to-code</strong> capabilities.</li>
                <li><strong>Sampling</strong> and <strong>iterative refinement</strong> improve results, with <strong>Claude 3.5 Sonnet</strong> reaching <strong>74.3% pass@1</strong> with 100 samples and <strong>55.3% pass@1</strong> with four self-refinement iterations.</li>
                <li>Models struggle with tasks <strong>trivial for humans</strong>, especially in <strong>spatial transformations</strong>, <strong>topological relationships</strong>, and <strong>dynamic patterns</strong>.</li>
              </ul>

            </div>
            <h2 class="title is-3">üõ† Benchmark Construction</h2>
            <div class="container is-max-desktop has-text-centered">
              <img src="static/images/construct_pipeline.png" alt="HumanEval-V Construction" width="100%">
              <p>We construct HumanEval-V following a <b>collect-distill-recreate-diversify</b> pipeline. After constructing the benchmark, <br> we perform rigorous validation to ensure that each coding task aligns with high-quality standards.
              </p>
            </div>
            <br>
            <h2 class="title is-3">üìÅDiagram Examples</h2>
            <div class="container is-max-desktop has-text-centered">
              <img src="static/images/our_diagrams.png" alt="HumanEval-V Examples" width="100%">
              <p>HumanEval-V includes visual elements like trees, graphs, matrices, maps, grids, flowcharts, and more. The visual contexts are designed to be <b>indispensable and self-explanatory</b>, embedding rich contextual information and algorithmic patterns.
              </p>
            </div>
            <br>
            <h2 class="title is-3">‚öô Evaluation Setting</h2>
            <div class="container is-max-desktop has-text-centered">
              <img src="static/images/evaluation_pipeline.png" alt="HumanEval-V Evaluation Pipelines" width="90%">
              <p>We use a structured <b>evaluation pipeline</b> to assess <b>visual reasoning</b> and <b>coding efficiency</b> separately,<br> ensuring that models' abilities are evaluated in a decoupled manner.</p>
            </div>
          </div>
        </div>
    </div>
    <div class="container">
      <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3" id="leaderboard">üèÜLeaderboardüèÜ</h2>
          
          <div class="model-labels-container">
            <span class="leaderboard-label open_weight">Open-Weight</span>
            <span class="leaderboard-label proprietary">Proprietary</span>
          </div>
          <br>
          <div class="content has-text-centered content">
            <p class="test-desc"> 
              The best performance is shown in <b>bold</b>, while the second-best is indicated by <u>underlining</u>. 
              You can sort <i>pass@1</i> or <i>pass@3</i> by clicking on the column headers.<br>
            </p>
          </div>
          <div class="leaderboard-container">
            <div class="table-wrapper">
              <table id="HEV-table">
                  <thead>
                      <tr>
                          <th rowspan="2" style="vertical-align: middle; width: 200px;">Models</th>
                          <th rowspan="2" style="vertical-align: middle; width: 50px;">Source</th>
                          <th colspan="2">V2C</th>
                          <th colspan="2">V2C w/ CoT</th>
                          <th colspan="2">V2T2C</th>
                          <th colspan="2">V2T2C w/ GPT-4o</th>
                      </tr>
                      <tr>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@1</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@3</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@1</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@3</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@1</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@3</th>
                          <th class="sortable clickable" style="width: 100px;" data-sort="number">pass@1</th>
                          <th class="sortable clickable sort-desc" style="width: 100px;" data-sort="number">pass@3</th>
                      </tr>
                  </thead>
                  <tbody>
                  </tbody>
              </table>
            </div>
        </div>

          <div class="content has-text-left content">
            <p class="test-desc">
            <ul>
              <li><i>pass@1</i> is computed using greedy decoding, whereas <i>pass@3</i> is based on 6 generated samples with sampling parameters of <i>t=0.8</i>, <i>topk=20</i> and <i>topp=0.95</i>.</li>
            </ul>
            </p>
          </div>
        </div>
      </div>
    </div>
    </section>
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
@article{zhang2024humanevalv,
  title={HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks}, 
  author={Zhang, Fengji and Wu, Linquan and Bai, Huiyu and Lin, Guancheng and Li, Xiao and Yu, Xiao and Wang, Yue and Chen, Bei and Keung, Jacky},
  journal={arXiv preprint arXiv:2410.12381},
  year={2024},
}
        </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <div class="visitor-counter">
              <p>Thank you for visiting! You are visitor #<span id="busuanzi_value_site_uv" class="visitor-number"></span>. Total page views: <span id="busuanzi_value_site_pv" class="view-number"></span>.</p>
              <p>
                This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mmmu-benchmark.github.io/">MMMU</a>, licensed under a <a rel="license"
                                                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
